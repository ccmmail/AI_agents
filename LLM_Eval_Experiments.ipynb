{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389a09e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.evaluation.scoring.eval_chain import ScoreStringResultOutputParser\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"scratch\"\n",
    "\n",
    "LLM = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2c46a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PROMPTS = {\n",
    "    \"Unusual\": \"What are the best unusual attractions and things to do in {city}?\",\n",
    "    \"Food\": \"What are the best local food to eat in {city}?\"\n",
    "}\n",
    "\n",
    "CRITERIA_TEXT_SCORE = (\n",
    "    \"a) **Correctness** – Does the answer provide recommendations that *match* the reference response?\\n\"\n",
    "    \"- 0: No overlap with reference ideas; mostly generic or irrelevant\\n\"\n",
    "    \"- 1: One idea overlaps partially with a reference point\\n\"\n",
    "    \"- 2: Multiple partial overlaps, but many important ideas missing\\n\"\n",
    "    \"- 3: Substantial overlap with reference, but some key omissions\\n\"\n",
    "    \"- 4: Near-complete coverage of specific reference items\\n\"\n",
    "    \"- 5: Covers all reference ideas, including less obvious ones\\n\"\n",
    "    \"\\n\"\n",
    "    \"b) **Interesting** – Are the recommendations offbeat or non-mainstream?\\n\"\n",
    "    \"- 0: Entirely standard tourist traps\\n\"\n",
    "    \"- 5: Entirely unusual, creative, or niche\\n\"\n",
    "    \"\\n\"\n",
    "    \"c) **Believability** – Does the response justify why each recommendation is interesting?\\n\"\n",
    "    \"- 0: No justification provided\\n\"\n",
    "    \"- 5: Every recommendation well-justified with insight\\n\"\n",
    "    \"\\n\"\n",
    "    \"Sum the three criterion scores (0–15) and map to a final score (1–10).\\n\"\n",
    "    \"Output each criterion score, and on a new line, output the final score wrapped in double brackets, e.g. [[9]].\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24137de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_response(input_text, prediction, reference, criteria_text):\n",
    "    prompt = (\n",
    "        f\"### Input\\n{input_text}\\n\\n\"\n",
    "        f\"### Submission\\n{prediction}\\n\\n\"\n",
    "        f\"### Reference\\n{reference}\\n\\n\"\n",
    "        f\"### Evaluation Criteria\\n{criteria_text}\\n\\n\"\n",
    "    )\n",
    "    response = LLM.invoke(prompt)\n",
    "    return ScoreStringResultOutputParser().parse(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b452638",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_text = PROMPTS[\"Unusual\"].format(city=\"Auckland\")\n",
    "submission = \"Auckland has many attractions including Sky Tower, harbor cruises, and Waiheke Island vineyards.\"\n",
    "reference = \"Explore lava caves, WWII tunnels, and behind-the-scenes tours in Auckland.\"\n",
    "\n",
    "criteria = CRITERIA_TEXT_SCORE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61299c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = evaluate_response(input_text, submission, reference, criteria)\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}